{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Building Learning Systems from Scratch: Notes and Mini-Projects","metadata":{}},{"cell_type":"markdown","source":"# MIT Maker Portfolio Verification\n# This notebook was created by Jazmine Gu\n# Original development on Kaggle: https://www.kaggle.com/code/happybookgiggles/simple-systems\n# Contact: jgu@raleighcharterhs.org\n","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents:","metadata":{}},{"cell_type":"markdown","source":"\n1. Learning Systems\n\n   1a. Linear Regression- predicting house prices from size\n\n   1b. Logistic regression- predicting email spam\n\n   1c. Small, multi-layer neural network — predicting exam pass/fail\n   \n2. Probabilistic / statistical models\n\n   2a. Naive Bayes — classifying news topic\n\n   2b. Gaussian Mixture Model — clustering height/weight\n\n   2c. Hidden Markov Model — weather inference\n\n3. Reinforcement Learning\n\n   3a. Q-learning — grid world navigation\n\n   3b. Policy iteration — stochastic portfolio optimization","metadata":{}},{"cell_type":"markdown","source":"# 1a. Linear regression — predicting house prices from size","metadata":{}},{"cell_type":"markdown","source":"I used linear regression to model the relationship between house size and price. I normalized the x-values to keep numerical stability, and then converted the parameters back to the original units.\n\n**Notes to self:**\n\n**What's the equation?**  \nThe predicted value for each input is:  \n$$\n\\hat{y}_i = w \\cdot x_i + b\n$$\nwhere $w$ is the slope and $b$ is the intercept.\n\n**How do we calculate loss, and update the parameters based on loss?**  \nUsing mean squared error loss:  \n$$\nL(w, b) = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2\n$$\nCalculating the gradients of the loss with respect to the parameters to see how changing each parameter affects the error:  \n$$\n\\frac{\\partial L}{\\partial w} = \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i) \\cdot x_i\n$$\n$$\n\\frac{\\partial L}{\\partial b} = \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y}_i - y_i)\n$$\nUpdating the parameters by subtracting the respective gradient multiplied by the learning rate, which reduces the loss:\n$$\nw := w - \\text{lr} \\cdot \\frac{\\partial L}{\\partial w}, \\quad\nb := b - \\text{lr} \\cdot \\frac{\\partial L}{\\partial b}\n$$\n\n**How do I know when to stop updating $w$ and $b$?**  \nSince this is a convex function, there is no chance of getting stuck in a local minimum instead of the global minimum. We can stop updating when the change in loss becomes smaller than a very small threshold, or after a maximum number of epochs if the threshold is never reached.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# DATA: house size (x) vs price (y)\nnp.random.seed(0)\nx = np.random.uniform(500, 3000, 100)  # square feet\ntrue_w, true_b = 150, 20000\ny = true_w * x + true_b + np.random.normal(0, 20000, size=len(x))\n\n# Preserve original values\nx_raw = x.copy()\n\n# Normalize x\nmu = x_raw.mean()\nsigma = x_raw.std()\nx = (x_raw - mu) / sigma\n\n# MODEL: y_hat = w*x + b\nw, b = 0.0, 0.0\nlr = 0.01\nn = len(x)\n\n# Gradient descent with convergence\ntolerance = 1e-6       # stop when improvement in loss is smaller than this\nmax_epochs = 100000      # prevent infinite loops\nprev_loss = float('inf')  # initial previous loss\n\nfor epoch in range(max_epochs):\n    y_hat = w * x + b\n    loss = np.mean((y_hat - y) ** 2)\n    \n    # Stop if improvement is smaller than tolerance\n    if abs(prev_loss - loss) < tolerance:\n        print(f\"Converged at epoch {epoch}\")\n        break\n\n    # Compute gradients\n    dw = (2/n) * np.sum((y_hat - y) * x)\n    db = (2/n) * np.sum(y_hat - y)\n\n    # Gradient descent\n    w -= lr * dw\n    b -= lr * db\n\n    prev_loss = loss\n\n# Convert parameters back to original units\nw_original = w / sigma\nb_original = b - (w * mu / sigma)\n\nprint(\"Slope:\", w_original)\nprint(\"Intercept:\", b_original)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-27T16:38:34.863706Z","iopub.execute_input":"2025-12-27T16:38:34.864101Z","iopub.status.idle":"2025-12-27T16:38:34.898754Z","shell.execute_reply.started":"2025-12-27T16:38:34.864072Z","shell.execute_reply":"2025-12-27T16:38:34.897602Z"}},"outputs":[{"name":"stdout","text":"Converged at epoch 889\nSlope: 149.49547780197474\nIntercept: 24695.281071959296\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# 1b. Logistic regression — predicting email spam","metadata":{}},{"cell_type":"markdown","source":"I used logistic regression to tell if an email is spam or not spam (1 or 0) based on the number of times a keyword appears.\n\n**Notes to self:**\n\n**Why use the sigmoid function?**  \nThe sigmoid function turns real values into values between 0 and 1, which is good because probabilities must be between 0 and 1.  \nThe linear combination  \n$$\nz = w \\cdot x + b\n$$\nrepresents the log-odds of the outcome. The sigmoid function converts this log-odds value into a probability:  \n$$\n\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n$$\nMathematically, larger $z$ means higher probabilities and smaller $z$ means lower probabilities, which is great. What's also great is that the sigmoid is continuous and differentiable, which makes it perfect for gradient-based optimization.\n\n**What are $w$ and $b$ for?**  \n$w$ is the weight for the feature $x$, which decides how much $x$ affects the probability of spam. A larger $|w|$ means the model is more sensitive to changes in $x$.  \n$b$ is intercept, shifting the sigmoid left or right, which comes in handy when $x=0$.\n\n**How is log loss used?**  \nThe log loss measures how well the predictions match the true labels:  \n- If $y = 1$, the loss contribution is $- \\log(\\hat{y})$  \n- If $y = 0$, the loss contribution is $- \\log(1 - \\hat{y})$  \n\nThis penalizes confident but wrong predictions more than less confident ones. The goal is that mean loss over all examples is minimized during training.\n\n**How do I check the model?**  \nWe have the final $w$ and $b$. For each input $x$ we calculate:  \n$$\nz = w \\cdot x + b, \\quad \\hat{y} = \\sigma(z)\n$$\nWe convert predicted probabilities into labels using a threshold:  \n$$\n\\text{predicted label} =\n\\begin{cases}\n1 & \\text{if } \\hat{y} \\ge 0.5 \\\\\n0 & \\text{if } \\hat{y} < 0.5\n\\end{cases}\n$$\nFinally, we compare predicted labels with the true labels to compute accuracy, which is the fraction of correct predictions.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# DATA: word frequency vs spam (0/1)\nx = np.array([1, 2, 3, 4, 5])  # keyword frequency\ny = np.array([0, 0, 0, 1, 1])  # labels: 0 = not spam, 1 = spam\n\n# Normalize feature for numerical stability\nx = (x - x.mean()) / x.std()\n\n# Sigmoid function\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n# Initial parameters\nw, b = 0.0, 0.0\nlr = 0.1\n\n# Gradient descent\nfor epoch in range(1000):\n    z = w * x + b\n    y_hat = sigmoid(z)\n\n    # log loss\n    loss = -np.mean(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n\n    # Gradients\n    dw = np.mean((y_hat - y) * x)\n    db = np.mean(y_hat - y)\n\n    # Update parameters\n    w -= lr * dw\n    b -= lr * db\n\n# Print final parameters\nprint(\"Final weight w:\", w)\nprint(\"Final bias b:\", b)\n\n# Check model; Use model to predict probabilities\nz = w * x + b\ny_hat = sigmoid(z)\n\n# Convert probabilities to labels\nlabels = (y_hat >= 0.5).astype(int)\n\n# Calculate accuracy\naccuracy = np.mean(labels == y)\nprint(\"Accuracy:\", accuracy)\n\n# Looking at predictions individually\nfor xi, yi, phat, pred in zip(x, y, y_hat, labels):\n    print(f\"x={xi:.2f}, true={yi}, predicted_prob={phat:.2f}, predicted_class={pred}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T17:22:02.771580Z","iopub.execute_input":"2025-12-27T17:22:02.771866Z","iopub.status.idle":"2025-12-27T17:22:02.833040Z","shell.execute_reply.started":"2025-12-27T17:22:02.771840Z","shell.execute_reply":"2025-12-27T17:22:02.831912Z"}},"outputs":[{"name":"stdout","text":"Final weight w: 5.137534570500907\nFinal bias b: -1.6746802995927876\nAccuracy: 1.0\nx=-1.41, true=0, predicted_prob=0.00, predicted_class=0\nx=-0.71, true=0, predicted_prob=0.00, predicted_class=0\nx=0.00, true=0, predicted_prob=0.16, predicted_class=0\nx=0.71, true=1, predicted_prob=0.88, predicted_class=1\nx=1.41, true=1, predicted_prob=1.00, predicted_class=1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 1c. Small, multi-layer neural network — predicting exam pass/fail","metadata":{}},{"cell_type":"markdown","source":"I used a small feedforward neural network with one hidden layer to map input features $x$ to a binary output $y$ (0 or 1) for predicting probabilities.\n\n**Notes to self:**\n\n\n**What are Neurons and Layers?**\n\nA neuron calculates:  \n$$\nz = w \\cdot x + b\n$$  \n$$\na = \\sigma(z)\n$$\n\nA layer is a group of neurons sharing the same input. \nThe output neuron takes hidden layer activations $a_i$ as input:  \n$$\nz_{\\text{out}} = v_1 a_1 + v_2 a_2 + v_3 a_3 + c\n$$  \n$$\n\\hat{y} = \\sigma(z_{\\text{out}})\n$$  \n\n\n**What do the Neurons and Layers do?**\n\nHidden layer:\n$$\nZ_1 = X W_1 + b_1\n$$  \n$$\nA_1 = \\sigma(Z_1)\n$$  \n\nOutput layer:  \n$$\nZ_2 = A_1 W_2 + b_2\n$$  \n$$\n\\hat{y} = \\sigma(Z_2)\n$$  \n\nTLDR: Each hidden neuron finds a different feature from the same input because their weights and biases differ and are updated independently. Each layer uses the outputs of the previous layer as inputs. This is why this is called a feedforward network.\n\n\n**How do the neurons learn?**\n\nWe use log loss:  \n$$\nL = -\\frac{1}{n} \\sum_{i=1}^{n} \\big( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\big)\n$$\n\nGradients are computed using the chain rule:\n\nOutput layer gradients:  \n$$\ndZ_2 = \\hat{y} - y\n$$  \n$$\ndW_2 = \\frac{A_1^T \\cdot dZ_2}{n}, \\quad\ndb_2 = \\text{mean}(dZ_2)\n$$\n\nHidden layer gradients: \n$$\ndA_1 = dZ_2 W_2^T\n$$  \n$$\ndZ_1 = dA_1 * A_1 * (1 - A_1)  \\quad \\text{(derivative of sigmoid)}\n$$  \n$$\ndW_1 = \\frac{X^T \\cdot dZ_1}{n}, \\quad\ndb_1 = \\text{mean}(dZ_1)\n$$\n\nGradient descent updates:  \n$$\nW := W - \\text{lr} \\cdot dW, \\quad\nb := b - \\text{lr} \\cdot db\n$$  \n\n\n**Why use multiple layers and non-linearity?**\n\n- Multiple layers allow hierarchical feature representations. (Ex: the first layer learns simple features and subsequent layers combine them into more complex patterns).  \n- Without non-linear activations, stacking layers collapses to a single linear transformation, so using sigmoid (or another non-linear activation) lets networks to learn more complex functions.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# DATA: simple binary classification\nx = np.array([1, 2, 3, 4, 5, 6]).reshape(-1, 1)  # inputs (hours studied) as column vector\ny = np.array([0, 0, 0, 1, 1, 1]).reshape(-1, 1)  # outputs (pass, fail (1, 0) as column vector\n\n# NETWORK STRUCTURE\ninput_size = 1\nhidden_size = 3  # number of neurons in hidden layer\noutput_size = 1  # single output neuron for binary classification\nlr = 0.1\nepochs = 2000\n\n# INITIALIZE WEIGHTS AND BIASES\n\n# Hidden layer weights and biases\nW1 = np.random.randn(input_size, hidden_size)\nb1 = np.zeros((1, hidden_size))\n\n# Output layer weights and bias\nW2 = np.random.randn(hidden_size, output_size)\nb2 = np.zeros((1, output_size))\n\n# SIGMOID FUNCTION\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n# TRAINING LOOP\nfor epoch in range(epochs):\n    # --- Forward pass ---\n    Z1 = x @ W1 + b1        # Linear combination for hidden layer\n    A1 = sigmoid(Z1)        # Activation (hidden layer outputs)\n    Z2 = A1 @ W2 + b2       # Linear combination for output layer\n    y_hat = sigmoid(Z2)     # Activation (output probability)\n\n    #  Compute gradients (backpropagation)\n    # Output layer gradient\n    dZ2 = y_hat - y                         # derivative of loss w.r.t Z2\n    dW2 = A1.T @ dZ2 / x.shape[0]           # gradient w.r.t W2\n    db2 = np.mean(dZ2, axis=0, keepdims=True)  # gradient w.r.t b2\n\n    # Hidden layer gradient\n    dA1 = dZ2 @ W2.T                        # propagate gradient to hidden layer\n    dZ1 = dA1 * A1 * (1 - A1)               # derivative of sigmoid\n    dW1 = x.T @ dZ1 / x.shape[0]\n    db1 = np.mean(dZ1, axis=0, keepdims=True)\n\n    # Update weights and biases\n    W2 -= lr * dW2\n    b2 -= lr * db2\n    W1 -= lr * dW1\n    b1 -= lr * db1\n\n# CHECK MODEL\n# Forward pass after training\nA1 = sigmoid(x @ W1 + b1)\ny_hat = sigmoid(A1 @ W2 + b2)\npredictions = (y_hat >= 0.5).astype(int)\naccuracy = np.mean(predictions == y)\nprint(\"Predictions:\", predictions.flatten())\nprint(\"Accuracy:\", accuracy)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T18:11:57.444324Z","iopub.execute_input":"2025-12-27T18:11:57.444599Z","iopub.status.idle":"2025-12-27T18:11:57.739516Z","shell.execute_reply.started":"2025-12-27T18:11:57.444573Z","shell.execute_reply":"2025-12-27T18:11:57.738114Z"}},"outputs":[{"name":"stdout","text":"Predictions: [0 0 0 1 1 1]\nAccuracy: 1.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 2a. Naive Bayes — classifying news topic","metadata":{}},{"cell_type":"markdown","source":"I used a Naive Bayes classifier to classify news topics in documents (sports (0) or politics (1)) based on the number of key words in the document.\n\n**Notes to self:**\n\n**What is prior probability?**  \nThe prior probability of a class, $P(C)$, is the probability that a document belongs to class $C$ before observing any features. In other words, it's the fraction of documents in each class:\nFor this dataset:\n    - Number of sports documents = 2\n    - Number of politics documents = 2\n    - Total documents = 4\nSo:\n$$P(\\text{sports}) = \\frac{2}{4} = 0.5$$  \n$$P(\\text{politics}) = \\frac{2}{4} = 0.5$$\n\n\n\n**What is Likelihood?**  \nThe likelihood is the probability of observing each feature given the class, denoted $P(x_i \\mid C)$. Naive Bayes assumes that features are conditionally independent given the class, which simplifies the likelihood for all features as:\n\n$$P(x \\mid C) = \\prod_{i} P(x_i \\mid C)^{x_i}$$\n\n$x_i$ is the count of feature $i$ in the document.  \nWe raise $P(x_i \\mid C)$ to the power of $x_i$ to account for multiple occurrences of the same word.\n\n\n**What is Laplace Smoothing?**  \n\nSometimes a word does not appear in the training documents for a class, which would give a probability of $0$. To avoid this, we use Laplace smoothing:\n\n$$P(x_i \\mid C) = \\frac{\\text{count of word } i \\text{ in class } C + 1}{\\text{total words in class } C + V}$$\n\n$V$ = number of features (here 2)\n\nUsing our dataset:\n\nSports (class 0): total words = 6  \n  - Word1: $(5 + 1) / (6 + 2) = 0.75$  \n  - Word2: $(1 + 1) / (6 + 2) = 0.25$  \n\nPolitics (class 1): total words = 8  \n  - Word1: $(1 + 1) / (8 + 2) = 0.2$  \n  - Word2: $(7 + 1) / (8 + 2) = 0.8$  \n\n\n\n**What is Posterior Probability?**  \n\nThe posterior probability is the probability of the class given the observed features:\n\n$$P(C \\mid x) = \\frac{P(C) \\cdot P(x \\mid C)}{P(x)}$$\n\n$P(C)$ is the prior probability.  \n$P(x \\mid C)$ is the likelihood.  \n$P(x)$ is the probability of observing the data, which is the same for all classes when choosing the most probable class, so we can ignore it for classification.\n\nRemember to calculate it in log space in code to avoid numerical underflow:\n\nWe select the class with the largest posterior probability (maximum a posteriori estimate).\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Dataset\n# Each row is a document\n# Each column is a word count\nX = np.array([\n    [3, 0],  # sports-heavy document\n    [2, 1],  # sports document\n    [0, 3],  # politics-heavy document\n    [1, 4]   # politics document\n])\n\n# Class labels: 0 = sports, 1 = politics\ny = np.array([0, 0, 1, 1])\n\n# Step 1: Compute prior probabilities\nclasses = np.unique(y)\npriors = {c: np.mean(y == c) for c in classes}\nprint(\"Priors:\", priors)\n\n# Step 2: Compute likelihoods (with Laplace smoothing)\nlikelihoods = {}\nV = X.shape[1]  # number of features\nfor c in classes:\n    X_c = X[y == c]  # select all documents of class c\n    likelihoods[c] = (X_c.sum(axis=0) + 1) / (X_c.sum() + V)  # Laplace smoothing\n\nprint(\"Likelihoods:\")\nfor c in likelihoods:\n    print(f\"Class {c}: {likelihoods[c]}\")\n\n# Step 3: Define prediction function\ndef predict(x):\n    posteriors = {}\n    for c in classes:\n        # log prior + sum of log likelihoods * feature counts\n        posteriors[c] = np.log(priors[c]) + np.sum(np.log(likelihoods[c]) * x)\n    # return class with highest posterior\n    return max(posteriors, key=posteriors.get)\n\n# Return posterior probabilities as well\ndef predict_with_probs(x):\n    posteriors = {}\n    for c in classes:\n        posteriors[c] = np.log(priors[c]) + np.sum(np.log(likelihoods[c]) * x)\n    # Convert log-posteriors to probabilities\n    max_log = max(posteriors.values())\n    exp_post = {c: np.exp(posteriors[c] - max_log) for c in posteriors}\n    total = sum(exp_post.values())\n    probs = {c: exp_post[c] / total for c in exp_post}\n    return max(probs, key=probs.get), probs\n\n# Step 4: Test predictions\ncorrect = 0\nfor xi, yi in zip(X, y):\n    pred = predict(xi)\n    print(f\"Document {xi} -> Predicted: {pred}, Actual: {yi}\")\n    if pred == yi:\n        correct += 1\n\naccuracy = correct / len(y)\nprint(\"Accuracy:\", accuracy)\n\n# Step 5: Test with posterior probabilities\ntest_doc = np.array([3, 1])\npred_class, pred_probs = predict_with_probs(test_doc)\nprint(f\"\\nTest document {test_doc} -> Predicted class: {pred_class}, Posterior probabilities: {pred_probs}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T18:53:59.199100Z","iopub.execute_input":"2025-12-27T18:53:59.199438Z","iopub.status.idle":"2025-12-27T18:53:59.215473Z","shell.execute_reply.started":"2025-12-27T18:53:59.199411Z","shell.execute_reply":"2025-12-27T18:53:59.214496Z"}},"outputs":[{"name":"stdout","text":"Priors: {np.int64(0): np.float64(0.5), np.int64(1): np.float64(0.5)}\nLikelihoods:\nClass 0: [0.75 0.25]\nClass 1: [0.2 0.8]\nDocument [3 0] -> Predicted: 0, Actual: 0\nDocument [2 1] -> Predicted: 0, Actual: 0\nDocument [0 3] -> Predicted: 1, Actual: 1\nDocument [1 4] -> Predicted: 1, Actual: 1\nAccuracy: 1.0\n\nTest document [3 1] -> Predicted class: 0, Posterior probabilities: {np.int64(0): np.float64(0.9427901000055868), np.int64(1): np.float64(0.05720989999441311)}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# 2b. Gaussian Mixture Model — clustering height/weight","metadata":{}},{"cell_type":"markdown","source":"I used a Gaussian Mixture Model to infer two subgroups of heights from a set of given heights, one around 160 cm and one around 180 cm. \n\n**Notes to self:** \n\n**What is a Gaussian?**\n\nA Gaussian is a fancy name for the normal distribution. The formula for the Gaussian probability density function is:\n\n$$\n\\text{gaussian}(x, \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n$$\n\n**What is Gaussian k?**\n\nGaussian $k$ is one component of the mixture. Each Gaussian has its own mean, standard deviation, and weight. You have to choose, as the programmer, how many Gaussians to include in the model. The parameters of each Gaussian are learned from the data using the EM algorithm.\n\n**What are mixture and responsibilities?**\n\nIt is assumed that the overall data comes from a mixture of Gaussians. This means each data point could come from any of the Gaussians, but we do not know which. Each Gaussian has a weight $\\pi_k$, which represents the fraction of points in that Gaussian.\n\nFor each data point $i$, we compute the responsibility $\\gamma_{ik}$, which is the probability that point $i$ comes from Gaussian $k$. This is also called a “soft assignment” because points are not assigned fully to one Gaussian—they are assigned fractionally. The formula is:\n\n$$\n\\gamma_{ik} = \\frac{\\pi_k \\, \\text{gaussian}(x_i, \\mu_k, \\sigma_k)}{\\sum_j \\pi_j \\, \\text{gaussian}(x_i, \\mu_j, \\sigma_j)}\n$$\n\nThe responsibility mass of Gaussian $k$ is the sum of all its responsibilities over all points. This represents the number of points “assigned” to that Gaussian.\n\n**What is the EM Algorithm?**\n\n**E-step (Expectation):**  \nCompute the responsibilities $\\gamma_{ik}$ for all points using the current guesses of $\\mu$, $\\sigma$, and $\\pi$. This tells us how much each point belongs to each Gaussian.\n\n**M-step (Maximization):**  \nUpdate the parameters to maximize the likelihood of the data given the responsibilities:\n\n$$\nN_k = \\sum_i \\gamma_{ik}\n$$\n\n$$\n\\mu_k = \\frac{\\sum_i \\gamma_{ik} x_i}{N_k}\n$$\n\n$$\n\\sigma_k = \\sqrt{\\frac{\\sum_i \\gamma_{ik} (x_i - \\mu_k)^2}{N_k}}\n$$\n\n$$\n\\pi_k = \\frac{N_k}{N}\n$$\n\n- $\\mu_k$ is the weighted average of points, weighted by their probability of belonging to this Gaussian.  \n- $\\sigma_k$ is the weighted standard deviation.  \n- $\\pi_k$ is the fraction of the population assigned to this Gaussian.  \n\nRepeat E-step and M-step until the parameters stop changing significantly, or until the likelihood of the data stops increasing.\n\n**What is likelihood?**\n\nThe likelihood measures how well the model explains the data. For a Gaussian mixture:\n\n$$\n\\mathcal{L}(\\theta) = \\prod_i \\sum_k \\pi_k \\, \\text{gaussian}(x_i, \\mu_k, \\sigma_k)\n$$\n\nEM increases this likelihood each iteration.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Synthetic data\ndata = np.concatenate([\n    np.random.normal(160, 5, 50),\n    np.random.normal(180, 5, 50)\n])\n\n# Initialize parameters\nmu = [155, 185]\nsigma = [5, 5]\npi = [0.5, 0.5]\n\ndef gaussian(x, mu, sigma):\n    return (1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-(x - mu)**2 / (2 * sigma**2))\n\n# EM algorithm\nmax_iters = 100\ntol = 1e-4  # convergence tolerance for parameter changes\n\nfor iteration in range(max_iters):\n    # Store old parameters\n    mu_old = mu.copy()\n    sigma_old = sigma.copy()\n    pi_old = pi.copy()\n    \n    # E-step\n    responsibilities = np.array([\n        pi[k] * gaussian(data, mu[k], sigma[k])\n        for k in range(2)\n    ])\n    responsibilities /= responsibilities.sum(axis=0)  # normalize\n    \n    # M-step\n    for k in range(2):\n        Nk = responsibilities[k].sum()\n        mu[k] = (responsibilities[k] * data).sum() / Nk\n        sigma[k] = np.sqrt(((data - mu[k])**2 * responsibilities[k]).sum() / Nk)\n        pi[k] = Nk / len(data)\n    \n    # Check convergence-stop if little change\n    mu_change = max(abs(mu[k] - mu_old[k]) for k in range(2))\n    sigma_change = max(abs(sigma[k] - sigma_old[k]) for k in range(2))\n    pi_change = max(abs(pi[k] - pi_old[k]) for k in range(2))\n    \n    if mu_change < tol and sigma_change < tol and pi_change < tol:\n        print(f\"Converged at iteration {iteration + 1}\")\n        break\n\n# Print final parameters\nprint(\"Final mu:\", mu)\nprint(\"Final sigma:\", sigma)\nprint(\"Final pi:\", pi)\n\nimport matplotlib.pyplot as plt\n\nx = np.linspace(140, 200, 500)\nplt.hist(data, bins=20, density=True, alpha=0.5, color='gray')\n\n# Plot fitted Gaussians\nfor k in range(2):\n    plt.plot(x, pi[k] * (1/(np.sqrt(2*np.pi)*sigma[k])) * np.exp(-(x-mu[k])**2 / (2*sigma[k]**2)), label=f'Gaussian {k+1}')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T20:31:36.868789Z","iopub.execute_input":"2025-12-27T20:31:36.869130Z","iopub.status.idle":"2025-12-27T20:31:37.097660Z","shell.execute_reply.started":"2025-12-27T20:31:36.869109Z","shell.execute_reply":"2025-12-27T20:31:37.096379Z"}},"outputs":[{"name":"stdout","text":"Converged at iteration 18\nFinal mu: [np.float64(159.5980450365633), np.float64(179.36925653066828)]\nFinal sigma: [np.float64(4.361404831189262), np.float64(5.18057736216083)]\nFinal pi: [np.float64(0.48126671676761357), np.float64(0.5187332832323864)]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# 2c. Hidden Markov Model — weather inference","metadata":{}},{"cell_type":"markdown","source":"I used a Hidden Markov Model (HMM) to infer hidden weather states (Rainy or Sunny) from whether people carry an umbrella or not ($0 =$ Umbrella, $1 =$ No Umbrella).\n\nHowever, because the dataset is small and the Baum–Welch algorithm maximizes likelihood without regularization, this model overfitted the observations.\n\n**Notes for self:**\n\n**What are the components of the model?**\n\nTransition Matrix ($A$):  \nThe Markov chain for the hidden states. Each entry\n\n$$\nA[i,j] = P(X_{t+1} = j \\mid X_t = i)\n$$\n\nrepresents the probability of transitioning from state $i$ to state $j$.\n\nEmission Matrix ($B$):  \nThe stochastic link between hidden states and observations:\n\n$$\nB[i,k] = P(O_t = k \\mid X_t = i)\n$$\n\nEven if the hidden state is Rainy, someone may or may not carry an umbrella. Unlike $A$, which governs hidden state transitions, $B$ governs the mapping from hidden states to observable data.\n\nInitial State Distribution ($\\pi$):\nGives the probability of the system starting in each hidden state:\n\n$$\n\\pi_i = P(X_1 = i)\n$$\n\n\n\n\n**What are Forward / Alpha Probabilities ($\\alpha_t(i)$)?**\n\nThe alpha probabilities represent the probability of being in hidden state $i$ at time $t$ and observing the sequence up to that point:\n\n$$\n\\alpha_1(i) = \\pi_i B[i, O_1]\n$$\n\n$$\n\\alpha_t(i) = \\left( \\sum_j \\alpha_{t-1}(j) A[j,i] \\right) B[i, O_t]\n$$\n\n$\\alpha$ is computed recursively; we do not know it in advance.  \nThe forward algorithm allows us to compute the sequence likelihood efficiently:\n\n$$\nP(O_1, \\dots, O_T) = \\sum_i \\alpha_T(i)\n$$\n\nWithout this recursion, we would have to list all possible sequences of hidden states, which grows exponentially with sequence length (for $T = 10$ and 2 states, $2^{10} = 1024$ sequences).\n\n\n\n\n**What is the Baum-Welch Algorithm (Expectation-Maximization)?**\n\nInitialization: \nStart with random $A$, $B$, and $\\pi$.\n\nE-step:\nCompute expected probabilities using forward ($\\alpha$) and backward ($\\beta$) probabilities:\n\n$$\n\\gamma_t(i) = P(X_t = i \\mid O, \\text{current model})\n$$\n\n$$\n\\xi_t(i,j) = P(X_t = i, X_{t+1} = j \\mid O, \\text{current model})\n$$\n\nM-step:  \nUpdate parameters based on expected counts:\n\n$$\n\\pi_i = \\gamma_1(i)\n$$\n\n$$\nA[i,j] = \\frac{\\sum_{t=1}^{T-1} \\xi_t(i,j)}{\\sum_{t=1}^{T-1} \\gamma_t(i)}\n$$\n\n$$\nB[i,k] = \\frac{\\sum_{t : O_t = k} \\gamma_t(i)}{\\sum_{t=1}^{T} \\gamma_t(i)}\n$$\n\nRepeat the E-step and M-step until convergence (the sequence likelihood stabilizes, ideally at a high value).\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Observed sequences (0=Umbrella, 1=No Umbrella)\n# Multiple sequences to improve learning\nsequences = [\n    [0,0,1,0,1,0],\n    [0,1,1,0,0,1],\n    [0,0,0,1,0,1],\n]\n\nnum_states = 2\nnum_obs = 2\n\n# Random initialization\nnp.random.seed(0)\nA = np.random.rand(num_states, num_states)\nA /= A.sum(axis=1, keepdims=True)\n\nB = np.random.rand(num_states, num_obs)\nB /= B.sum(axis=1, keepdims=True)\n\npi = np.random.rand(num_states)\npi /= pi.sum()\n\n# Baum-Welch helper functions\ndef forward(O, A, B, pi):\n    T = len(O)\n    alpha = np.zeros((T, num_states))\n    alpha[0] = pi * B[:, O[0]]\n    for t in range(1, T):\n        alpha[t] = (alpha[t-1] @ A) * B[:, O[t]]\n    return alpha\n\ndef backward(O, A, B):\n    T = len(O)\n    beta = np.zeros((T, num_states))\n    beta[-1] = 1\n    for t in reversed(range(T-1)):\n        beta[t] = A @ (B[:, O[t+1]] * beta[t+1])\n    return beta\n\n# Baum-Welch iterations\nfor iteration in range(20):\n    A_num = np.zeros_like(A)\n    A_den = np.zeros(num_states)\n    B_num = np.zeros_like(B)\n    B_den = np.zeros(num_states)\n    pi_new = np.zeros_like(pi)\n    \n    for O in sequences:\n        T = len(O)\n        alpha = forward(O, A, B, pi)\n        beta = backward(O, A, B)\n        likelihood = alpha[-1].sum()\n        \n        gamma = (alpha * beta) / likelihood\n        xi = np.zeros((T-1, num_states, num_states))\n        for t in range(T-1):\n            xi[t] = (np.outer(alpha[t], beta[t+1] * B[:, O[t+1]]) * A) / likelihood\n        \n        pi_new += gamma[0]\n        A_num += xi.sum(axis=0)\n        A_den += gamma[:-1].sum(axis=0)\n        \n        for k in range(num_obs):\n            mask = [t for t in range(T) if O[t]==k]\n            if mask:\n                B_num[:,k] += gamma[mask].sum(axis=0)\n        B_den += gamma.sum(axis=0)\n    \n    # Update parameters\n    pi = pi_new / pi_new.sum()\n    A = A_num / A_den[:,None]\n    B = B_num / B_den[:,None]\n\nprint(\"Learned transition matrix A:\")\nprint(A)\nprint(\"Learned emission matrix B:\")\nprint(B)\nprint(\"Learned initial probabilities pi:\")\nprint(pi)\n\n# Verify accuracy: compute likelihood of sequences\ntotal_likelihood = 1\nfor O in sequences:\n    alpha = forward(O, A, B, pi)\n    total_likelihood *= alpha[-1].sum()\nprint(\"Total likelihood of observed sequences:\", total_likelihood)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T21:05:53.745562Z","iopub.execute_input":"2025-12-27T21:05:53.745849Z","iopub.status.idle":"2025-12-27T21:05:53.816385Z","shell.execute_reply.started":"2025-12-27T21:05:53.745821Z","shell.execute_reply":"2025-12-27T21:05:53.815061Z"}},"outputs":[{"name":"stdout","text":"Learned transition matrix A:\n[[0.37432754 0.62567246]\n [0.78632008 0.21367992]]\nLearned emission matrix B:\n[[9.99977167e-01 2.28328440e-05]\n [3.23085899e-02 9.67691410e-01]]\nLearned initial probabilities pi:\n[1.00000000e+00 4.84886677e-13]\nTotal likelihood of observed sequences: 9.276426078874518e-05\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 3a. Q-learning — grid world navigation","metadata":{}},{"cell_type":"markdown","source":"I used tabular Q-learning with epsilon-greedy exploration to train an agent to navigate a number-line on the integers, starting on $0$ and ending at $4$. The agent can move left ($-1$) or right ($+1$) at each step, and receives a reward of $1$ for reaching the goal and $-0.01$ for all other steps.\n\n\n**Notes for self:**\n\n**What are the components of the model?**\n\nStates ($S$):\n$$S = \\{0, 1, 2, 3, 4\\}$$\n\nActions ($A$):\n$$A = \\{-1, +1\\}$$\n\nTransition function ($P(s_{t+1} \\mid s_t, a_t)$):\n$$s_{t+1} = \\min(4, \\max(0, s_t + a_t))$$\n\nReward function ($R(s_t,a_t)$):\n$$R(s_t, a_t) = \\begin{cases} \n1 & \\text{if } s_{t+1} = 4 \\\\\n-0.01 & \\text{otherwise}\n\\end{cases}$$\n\nDiscount factor ($\\gamma$):\n$$\\gamma = 0.9$$\n\nLearning rate ($\\alpha$):\n$$\\alpha = 0.1$$\n\nExploration rate ($\\epsilon$):** Probability of selecting a random action:\n$$\\epsilon = 0.1$$\n\n**What is the Q-value table ($Q(s,a)$)?**\n\n$Q(s,a)$ stores the agent's estimate of the expected discounted long-term reward for taking action $a$ in state $s$:\n\n$$Q(s,a) \\approx \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_t \\mid s_0 = s, a_0 = a\\right]$$\n\n**How are Q-values updated?**\n\nThe update rule is:\n\n$$Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha\\left(R(s_t, a_t) + \\gamma \\max_{a' \\in A} Q(s_{t+1}, a') - Q(s_t, a_t)\\right)$$\n\nWhere:\n\n$R(s_t,a_t)$ = observed reward after taking action $a_t$ in state $s_t$\n\n$s_{t+1} = \\min\\Big(4, \\max(0, s_t + a_t)\\Big)$\n\n$\\max_{a' \\in A} Q(s_{t+1}, a')$ = estimate of the best future reward from $s_{t+1}$\n\n**How does epsilon-greedy exploration work?**\n\nAt each step $t$, the action $a_t$ is chosen according to:\n\n$$a_t = \\begin{cases}\n\\text{random action from } A & \\text{with probability } \\epsilon \\\\\n\\arg\\max_{a \\in A} Q(s_t, a) & \\text{with probability } 1-\\epsilon\n\\end{cases}$$\n\nThis balances exploration (trying new actions) and exploitation (choosing the currently best-known action).\n\nWithout exploration, the agent may never discover the optimal path, especially in stochastic or larger environments.\n\nWith epsilon-greedy, the agent gradually learns the optimal Q-values through repeated episodes.\n\n**How does the agent learn?**\n\nInitialize $$Q(s,a) = 0$$ for all $$s \\in S, a \\in A$$\n\nAt each step, select $a_t$ using epsilon-greedy.\n\nObserve $s_{t+1}$ and $R(s_t,a_t)$.\n\nUpdate $Q(s_t,a_t)$ using the update equation above.\n\nRepeat for many episodes; the Q-values converge toward the expected discounted reward.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Grid-world setup\nnum_states = 5\nactions = [-1, 1]  # left, right\nnum_actions = len(actions)\n\n# Q-table\nQ = np.zeros((num_states, num_actions))\n\n# Hyperparameters\nalpha = 0.1        # learning rate\ngamma = 0.9        # discount factor\nepsilon = 0.1      # probability of exploring\nnum_episodes = 500\n\n# Track episode lengths to see learning progress\nepisode_lengths = []\n\nfor episode in range(num_episodes):\n    state = 0\n    steps = 0\n\n    while state != 4:  # until goal\n        # Epsilon-greedy action selection\n        if np.random.rand() < epsilon:\n            action = np.random.choice(actions)  # explore\n        else:\n            action_idx = np.argmax(Q[state])    # exploit best\n            action = actions[action_idx]\n\n        action_idx = 0 if action == -1 else 1\n        next_state = min(num_states-1, max(0, state + action))\n        reward = 1 if next_state == 4 else -0.01\n\n        # Q-learning update\n        Q[state, action_idx] += alpha * (\n            reward + gamma * Q[next_state].max() - Q[state, action_idx]\n        )\n\n        state = next_state\n        steps += 1\n\n    episode_lengths.append(steps)\n\n    # Print Q-table every 100 episodes\n    if (episode+1) % 100 == 0:\n        print(f\"Episode {episode+1} Q-table:\")\n        print(Q)\n        print(f\"Episode length: {steps}\\n\")\n\n# Final Q-table and summary\nprint(\"Final Q-table:\")\nprint(Q)\nprint(f\"Average episode length: {np.mean(episode_lengths):.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T21:51:41.228651Z","iopub.execute_input":"2025-12-27T21:51:41.228819Z","iopub.status.idle":"2025-12-27T21:51:41.308219Z","shell.execute_reply.started":"2025-12-27T21:51:41.228799Z","shell.execute_reply":"2025-12-27T21:51:41.307422Z"}},"outputs":[{"name":"stdout","text":"Episode 100 Q-table:\n[[0.18306832 0.69776224]\n [0.29139507 0.78968097]\n [0.1280147  0.88974349]\n [0.16665092 0.99997344]\n [0.         0.        ]]\nEpisode length: 4\n\nEpisode 200 Q-table:\n[[0.33383804 0.70189939]\n [0.40460548 0.79099993]\n [0.42737012 0.88999999]\n [0.49236621 1.        ]\n [0.         0.        ]]\nEpisode length: 6\n\nEpisode 300 Q-table:\n[[0.49779055 0.7019    ]\n [0.49351194 0.791     ]\n [0.52178094 0.89      ]\n [0.67530315 1.        ]\n [0.         0.        ]]\nEpisode length: 4\n\nEpisode 400 Q-table:\n[[0.54040645 0.7019    ]\n [0.52825361 0.791     ]\n [0.61574961 0.89      ]\n [0.706657   1.        ]\n [0.         0.        ]]\nEpisode length: 4\n\nEpisode 500 Q-table:\n[[0.55585413 0.7019    ]\n [0.57204344 0.791     ]\n [0.66852358 0.89      ]\n [0.74617667 1.        ]\n [0.         0.        ]]\nEpisode length: 4\n\nFinal Q-table:\n[[0.55585413 0.7019    ]\n [0.57204344 0.791     ]\n [0.66852358 0.89      ]\n [0.74617667 1.        ]\n [0.         0.        ]]\nAverage episode length: 4.43\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 3b. Policy iteration — stochastic portfolio optimization","metadata":{}},{"cell_type":"markdown","source":"I used policy iteration on a stochastic portfolio optimization problem to find an optimal investment strategy. The portfolio can be in one of five states (portfolio values from low to high), and at each state, the agent chooses between two actions: aggressive investment (higher expected return and variance) or conservative investment (lower return, lower variance). The model improves its policy to maximize expected discounted portfolio gains over time.\n\n**Notes for self:**\n\n**What are the components of the model?**\n\nStates ($S$): Discretized portfolio values representing current wealth.\n\nActions ($A$): Investment strategies in each state ($0 =$ aggressive, $1 =$ conservative).\n\nTransition probabilities ($P[s'|s,a]$): Probability of moving from state $s$ to $s'$ given action $a$. In this model, transitions are stochastic, estimated via Monte Carlo simulation of random returns.\n\nRewards ($R(s)$): Immediate gain in portfolio value when in state $s$. Computed as $R = \\text{next portfolio value} - \\text{current value}$.\n\nDiscount factor ($\\gamma$): Determines how much future rewards are worth relative to immediate rewards, $0 < \\gamma < 1$.\n\n**What is a policy ($\\pi$)?**\n\nA policy maps each state to an action: $\\pi(s) = a$.\n\nIt is initially random.\n\nPolicy iteration determines the policy by:\n1. Evaluating the current policy's value function $V^\\pi(s)$.\n2. Updating $\\pi(s)$ to choose the action maximizing expected future reward.\n3. Repeating until the policy stabilizes (optimal policy $\\pi^*$).\n\n**What is the value function and the Bellman expectation equation?**\n\nValue function ($V^\\pi(s)$): Expected total discounted reward starting from state $s$ following policy $\\pi$:\n\n$$V^\\pi(s) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t) \\mid \\pi\\right]$$\n\nBellman expectation equation: Recursively expresses $V^\\pi$ in terms of immediate reward and expected future value:\n\n$$V^\\pi(s) = R(s) + \\gamma \\sum_{s'} P(s' \\mid s, \\pi(s)) V^\\pi(s')$$\n\nComponents:\n- $R(s)$: immediate reward\n- $\\gamma$: discount factor\n- $P(s'|s,\\pi(s))$: probability of transitioning to $s'$ under action $\\pi(s)$\n\nFixed point: The value function $V^\\pi$ that satisfies this equation. Iteratively applying the Bellman update converges to this $V^\\pi$.\n\n**How does the model \"train\"?**\n\n1. Policy evaluation: Compute expected future rewards under current policy (Monte Carlo approximation of $V^\\pi$).\n2. Policy improvement: Update the policy to maximize $V^\\pi$ in each state.\n3. Repeat until the policy stops changing.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# PARAMETERS\nnp.random.seed(0)\n\nnum_states = 5  # portfolio bins: 0=very low, 4=very high\nnum_actions = 2  # 0=aggressive, 1=conservative\ngamma = 0.9  # discount factor\n\n# Discretized portfolio values for states\nportfolio_bins = np.linspace(1000, 5000, num_states)\n\n# Random return simulation per action\ndef simulate_return(action):\n    \"\"\"\n    Simulates the portfolio return given an action.\n    action 0 = aggressive (higher mean, higher variance)\n    action 1 = conservative (lower mean, lower variance)\n    \"\"\"\n    if action == 0:\n        return np.random.normal(1.05, 0.10)  # aggressive: +5% avg, 10% std\n    else:\n        return np.random.normal(1.02, 0.03)  # conservative: +2% avg, 3% std\n\n# INITIAL POLICY AND VALUE FUNCTION\npolicy = np.random.choice(num_actions, size=num_states)  # random start\nV = np.zeros(num_states)\n\nprint(\"Initial policy:\", policy)\n\n# POLICY ITERATION\nnum_iterations = 20\n\nfor iteration in range(num_iterations):\n    # --- Policy Evaluation ---\n    for eval_iter in range(50):  # inner loop to converge V\n        V_new = np.zeros(num_states)\n        for s, val in enumerate(portfolio_bins):\n            a = policy[s]\n            expected_rewards = []\n            # Monte Carlo estimate of expected next-state value\n            for _ in range(100):\n                next_val = val * simulate_return(a)\n                # map next_val to nearest state bin\n                next_state = np.argmin(np.abs(portfolio_bins - next_val))\n                reward = next_val - val  # reward = gain in portfolio\n                expected_rewards.append(reward + gamma * V[next_state])\n            V_new[s] = np.mean(expected_rewards)\n        V = V_new\n\n    # Policy Improvement\n    policy_stable = True\n    for s, val in enumerate(portfolio_bins):\n        action_values = []\n        for a in range(num_actions):\n            sim_rewards = []\n            for _ in range(100):\n                next_val = val * simulate_return(a)\n                next_state = np.argmin(np.abs(portfolio_bins - next_val))\n                reward = next_val - val\n                sim_rewards.append(reward + gamma * V[next_state])\n            action_values.append(np.mean(sim_rewards))\n        best_action = np.argmax(action_values)\n        if best_action != policy[s]:\n            policy_stable = False\n        policy[s] = best_action\n\n    print(f\"Iteration {iteration+1}: Policy = {policy}, V = {np.round(V,2)}\")\n    if policy_stable:\n        print(\"Policy has converged.\")\n        break\n\n# SIMULATE AN EPISODE\nstate_value = 3000.0  # start from initial portfolio\ntrajectory_values = [state_value]\ntotal_discounted_reward = 0.0\n\nfor t in range(10):\n    action = policy[np.argmin(np.abs(portfolio_bins - state_value))]  # choose action based on nearest bin\n    # simulate stochastic return\n    if action == 0:  # aggressive\n        return_multiplier = np.random.normal(1.05, 0.10)  # mean 5%, std 10%\n    else:  # conservative\n        return_multiplier = np.random.normal(1.02, 0.03)  # mean 2%, std 3%\n    next_value = state_value * return_multiplier\n    reward = next_value - state_value\n    total_discounted_reward += (gamma**t) * reward\n    state_value = next_value\n    trajectory_values.append(state_value)\n\nprint(\"Simulated portfolio trajectory:\", np.round(trajectory_values, 2))\nprint(\"Total discounted gain:\", round(total_discounted_reward, 2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-27T22:37:47.480915Z","iopub.execute_input":"2025-12-27T22:37:47.482026Z","iopub.status.idle":"2025-12-27T22:37:47.850819Z","shell.execute_reply.started":"2025-12-27T22:37:47.481983Z","shell.execute_reply":"2025-12-27T22:37:47.849521Z"}},"outputs":[{"name":"stdout","text":"Initial policy: [0 1 1 0 1]\nIteration 1: Policy = [0 0 0 0 0], V = [ 525.78  379.75  614.26 1171.37  981.88]\nIteration 2: Policy = [0 0 0 0 0], V = [ 519.01 1043.37 1781.8  2043.16 2231.9 ]\nPolicy has converged.\nSimulated portfolio trajectory: [3000.   3409.56 3428.05 3616.06 3659.83 3175.47 3703.15 3623.1  4043.19\n 3925.32 4463.33]\nTotal discounted gain: 920.28\n","output_type":"stream"}],"execution_count":3}]}